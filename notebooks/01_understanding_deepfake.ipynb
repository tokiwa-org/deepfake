{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake技術を理解する\n",
    "\n",
    "このノートブックは、教育目的でDeepfake技術の基本概念を解説します。\n",
    "\n",
    "## 概要\n",
    "\n",
    "Deepfake技術は**オートエンコーダ**を使用して2人の顔を交換します。重要なポイント：\n",
    "\n",
    "1. **共有エンコーダ**が普遍的な顔の特徴（表情、ポーズ、照明）を学習\n",
    "2. **人物別デコーダ**が個々の顔を再構築する方法を学習\n",
    "3. **顔交換**は、ある人の顔をエンコードし、別の人のデコーダでデコードすることで実現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セットアップ\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# デバイスの確認\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('MPS (Metal Performance Shaders) を使用 - M4 Mac GPU')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA GPUを使用')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CPUを使用')\n",
    "\n",
    "print(f'PyTorch バージョン: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. オートエンコーダとは？\n",
    "\n",
    "オートエンコーダは以下を行うニューラルネットワークです：\n",
    "1. 入力を圧縮された表現（潜在空間）に**エンコード**\n",
    "2. その表現を元に戻すように**デコード**\n",
    "\n",
    "```\n",
    "入力画像 (128x128x3) → エンコーダ → 潜在空間 (4x4x128) → デコーダ → 出力 (128x128x3)\n",
    "```\n",
    "\n",
    "ネットワークは圧縮と再構築を学習し、意味のある特徴を学ぶことを強制されます。\n",
    "\n",
    "### 軽量版と本番版の違い\n",
    "\n",
    "| 設定 | 軽量版（本プロジェクト） | 本番版 |\n",
    "|-----|----------------------|-------|\n",
    "| パラメータ数 | ~670万 | ~5,000万〜2億 |\n",
    "| 画像サイズ | 128×128 | 256×256〜512×512 |\n",
    "| 潜在次元 | 128 | 512〜1024 |\n",
    "| 学習時間/epoch | ~15秒 | ~10〜30分 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルなオートエンコーダの例\n",
    "class SimpleEncoder(nn.Module):\n",
    "    \"\"\"デモ用の簡略化されたエンコーダ\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),   # 128 -> 64\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # 64 -> 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # 32 -> 16\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    \"\"\"デモ用の簡略化されたデコーダ\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), # 16 -> 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 32 -> 64\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),   # 64 -> 128\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# テスト\n",
    "encoder = SimpleEncoder()\n",
    "decoder = SimpleDecoder()\n",
    "\n",
    "test_input = torch.randn(1, 3, 128, 128)  # 軽量版は128x128\n",
    "latent = encoder(test_input)\n",
    "output = decoder(latent)\n",
    "\n",
    "print(f'入力形状: {test_input.shape}')\n",
    "print(f'潜在空間形状: {latent.shape}')\n",
    "print(f'出力形状: {output.shape}')\n",
    "print(f'\\n圧縮率: {test_input.numel() / latent.numel():.1f}倍')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deepfakeアーキテクチャ\n",
    "\n",
    "Deepfakeの革新的なポイントは、**1つの共有エンコーダ**と**2つの別々のデコーダ**を使用することです：\n",
    "\n",
    "```\n",
    "人物Aの顔 ─┐\n",
    "           ├─> 共有エンコーダ ─> 潜在空間 ─┬─> デコーダA ─> 再構築されたA\n",
    "人物Bの顔 ─┘                              └─> デコーダB ─> 再構築されたB\n",
    "```\n",
    "\n",
    "### 学習時：\n",
    "- エンコーダ + デコーダA は人物Aの顔で学習\n",
    "- エンコーダ + デコーダB は人物Bの顔で学習\n",
    "- エンコーダは普遍的な特徴（表情、ポーズ）を学習\n",
    "- 各デコーダは人物固有の特徴（顔の形、肌の色）を学習\n",
    "\n",
    "### 顔交換時：\n",
    "- 人物Aをエンコード → 潜在表現（表情/ポーズ）を取得\n",
    "- デコーダBでデコード → 人物Aの表情を持つ人物Bの顔！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実際のモデルを読み込む\n",
    "from src.models import DeepfakeModel\n",
    "\n",
    "# 軽量版モデル（128x128、潜在次元128）\n",
    "model = DeepfakeModel(input_size=128, latent_dim=128).to(device)\n",
    "\n",
    "# パラメータをカウント\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "decoder_a_params = sum(p.numel() for p in model.decoder_a.parameters())\n",
    "decoder_b_params = sum(p.numel() for p in model.decoder_b.parameters())\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print('=== モデルアーキテクチャ ===')\n",
    "print(f'エンコーダ パラメータ: {encoder_params:,}')\n",
    "print(f'デコーダA パラメータ: {decoder_a_params:,}')\n",
    "print(f'デコーダB パラメータ: {decoder_b_params:,}')\n",
    "print(f'合計 パラメータ: {total_params:,}')\n",
    "print(f'\\n注: デコーダは同じアーキテクチャなので、パラメータ数も同じ')\n",
    "print(f'\\n参考: 本番レベルのDeepfakeは5,000万〜2億パラメータが一般的')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 潜在空間を理解する\n",
    "\n",
    "**潜在空間**こそがマジックが起こる場所です。これは以下を捉えた圧縮表現です：\n",
    "- 顔の表情とポーズ\n",
    "- 人物に依存しない情報（共有エンコーダのおかげ）\n",
    "- どのデコーダでもデコードでき、そのデコーダの人物の顔が得られる\n",
    "\n",
    "イメージとしては：\n",
    "- 潜在空間は「顔がどう見えるべきか」を保存（笑顔、目を開けている、頭の角度）\n",
    "- 各デコーダは「この人物の顔はどう見えるか」を知っている\n",
    "- 組み合わせると：「この表情の人物Bを見せて」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顔交換のコンセプトをデモ\n",
    "# ダミーの「顔」画像を作成（実際には本物の顔を使用）\n",
    "\n",
    "# 人物Aをシミュレート（顔を表すランダムパターン）\n",
    "face_a = torch.rand(1, 3, 128, 128).to(device)  # 軽量版は128x128\n",
    "\n",
    "# 人物Aの顔をエンコード\n",
    "latent_a = model.encoder(face_a)\n",
    "print(f'人物Aの顔を潜在空間にエンコード: {latent_a.shape}')\n",
    "\n",
    "# デコーダAでデコード（再構築）\n",
    "recon_a = model.decoder_a(latent_a)\n",
    "print(f'再構築されたA（同じデコーダ）: {recon_a.shape}')\n",
    "\n",
    "# デコーダBでデコード（顔交換！）\n",
    "swap_a_to_b = model.decoder_b(latent_a)\n",
    "print(f'Bに交換（別のデコーダ）: {swap_a_to_b.shape}')\n",
    "\n",
    "print('\\n=== これがDeepfakeの核心です！ ===')\n",
    "print('同じ潜在コード（Aの表情） → 異なるデコーダ（Bの顔）')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 損失関数\n",
    "\n",
    "モデルの学習には複数の損失を使用します：\n",
    "\n",
    "### 4.1 再構築損失 (L1)\n",
    "- ピクセル単位の差分\n",
    "- シンプルだが効果的\n",
    "- L1はL2よりシャープな画像を生成\n",
    "\n",
    "### 4.2 知覚損失 (Perceptual Loss / VGG)\n",
    "- VGGネットワークの高レベル特徴を比較\n",
    "- 構造的な類似性を捉える\n",
    "- 顔をより自然に見せる\n",
    "- **注意**: 計算コストが高いため、軽量版ではデフォルトで無効\n",
    "\n",
    "### 4.3 合計損失\n",
    "```\n",
    "L_total = λ_recon * L_recon + λ_perceptual * L_perceptual\n",
    "```\n",
    "\n",
    "| 損失関数 | 軽量版 | 本番版 |\n",
    "|---------|-------|-------|\n",
    "| L1 | ✓ | ✓ |\n",
    "| Perceptual | - | ✓ |\n",
    "| GAN | - | 推奨 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.losses import ReconstructionLoss, CombinedLoss\n",
    "\n",
    "# 損失関数を作成\n",
    "recon_loss = ReconstructionLoss()\n",
    "\n",
    "# 軽量版（L1のみ）\n",
    "combined_loss_fast = CombinedLoss(use_perceptual=False).to(device)\n",
    "\n",
    "# 品質重視版（Perceptual損失あり）\n",
    "# combined_loss_quality = CombinedLoss(use_perceptual=True).to(device)\n",
    "\n",
    "# ダミー画像でテスト\n",
    "pred = torch.rand(2, 3, 128, 128).to(device)\n",
    "target = torch.rand(2, 3, 128, 128).to(device)\n",
    "\n",
    "print('=== 損失値（ランダム画像なので高い損失が予想される） ===')\n",
    "print(f'再構築損失: {recon_loss(pred, target):.4f}')\n",
    "\n",
    "losses = combined_loss_fast(pred, target)\n",
    "print(f'\\n合計損失（軽量版、L1のみ）:')\n",
    "for k, v in losses.items():\n",
    "    print(f'  {k}: {v:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習プロセス\n",
    "\n",
    "学習ループ：\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for img_a, img_b in dataloader:\n",
    "        # 順伝播\n",
    "        latent_a = encoder(img_a)\n",
    "        latent_b = encoder(img_b)\n",
    "        recon_a = decoder_a(latent_a)\n",
    "        recon_b = decoder_b(latent_b)\n",
    "        \n",
    "        # 損失\n",
    "        loss = loss_fn(recon_a, img_a) + loss_fn(recon_b, img_b)\n",
    "        \n",
    "        # 逆伝播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "重要なポイント：\n",
    "- 各バッチで両方の人物の画像を処理\n",
    "- エンコーダは両方のデコーダからの勾配で更新される\n",
    "- 各デコーダは自分の人物からの勾配のみを受け取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1つの学習ステップをシミュレート\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ダミーバッチ（軽量版は128x128）\n",
    "img_a = torch.rand(4, 3, 128, 128).to(device)\n",
    "img_b = torch.rand(4, 3, 128, 128).to(device)\n",
    "\n",
    "# 順伝播\n",
    "recon_a, recon_b = model(img_a, img_b)\n",
    "\n",
    "# 損失\n",
    "loss_a = combined_loss_fast(recon_a, img_a)['total']\n",
    "loss_b = combined_loss_fast(recon_b, img_b)['total']\n",
    "total_loss = loss_a + loss_b\n",
    "\n",
    "# 逆伝播\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "\n",
    "# 勾配を確認\n",
    "encoder_has_grad = any(p.grad is not None for p in model.encoder.parameters())\n",
    "decoder_a_has_grad = any(p.grad is not None for p in model.decoder_a.parameters())\n",
    "\n",
    "print('=== 学習ステップ完了 ===')\n",
    "print(f'損失 A: {loss_a:.4f}')\n",
    "print(f'損失 B: {loss_b:.4f}')\n",
    "print(f'合計損失: {total_loss:.4f}')\n",
    "print(f'\\nエンコーダに勾配あり: {encoder_has_grad}')\n",
    "print(f'デコーダAに勾配あり: {decoder_a_has_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 概念のまとめ\n",
    "\n",
    "### 主要な概念：\n",
    "\n",
    "1. **オートエンコーダ**: 画像を圧縮し再構築\n",
    "2. **共有エンコーダ**: 普遍的な顔の特徴を学習\n",
    "3. **人物別デコーダ**: 個々の顔の特徴を学習\n",
    "4. **顔交換**: Aをエンコード + Bでデコード = Aの表情を持つBの顔\n",
    "\n",
    "### なぜこれが機能するのか：\n",
    "\n",
    "- エンコーダは両方のデコーダに役立つ特徴を学ぶことを強制される\n",
    "- これらの特徴は人物に依存しない（表情、ポーズ）\n",
    "- 各デコーダはこれらの特徴を特定の顔にマッピングする方法を学ぶ\n",
    "\n",
    "---\n",
    "\n",
    "以下のセクションでは、実際にデータ準備から顔交換テストまでを行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. データの準備と確認\n",
    "\n",
    "実際に顔交換を行うには、2人の顔画像が必要です。\n",
    "\n",
    "### データ構造\n",
    "```\n",
    "data/\n",
    "├── person_a/    # 人物Aの顔画像（500枚以上推奨）\n",
    "│   ├── 001.jpg\n",
    "│   ├── 002.jpg\n",
    "│   └── ...\n",
    "└── person_b/    # 人物Bの顔画像（500枚以上推奨）\n",
    "    ├── 001.jpg\n",
    "    ├── 002.jpg\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "### データセットの取得方法\n",
    "1. **FFHQ Dataset** (Kaggle): 高品質な顔画像\n",
    "2. **自分で撮影**: 同意を得た人物の顔を様々な角度で撮影\n",
    "3. **動画から抽出**: ffmpegで顔フレームを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データディレクトリの確認\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision import transforms\n",
    "\n",
    "data_a_dir = Path('../data/person_a')\n",
    "data_b_dir = Path('../data/person_b')\n",
    "\n",
    "def count_images(directory):\n",
    "    \"\"\"ディレクトリ内の画像数をカウント\"\"\"\n",
    "    if not directory.exists():\n",
    "        return 0\n",
    "    extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    return len([f for f in directory.iterdir() if f.suffix.lower() in extensions])\n",
    "\n",
    "print('=== データセット確認 ===')\n",
    "print(f'人物A: {count_images(data_a_dir)} 枚')\n",
    "print(f'人物B: {count_images(data_b_dir)} 枚')\n",
    "\n",
    "if count_images(data_a_dir) == 0 or count_images(data_b_dir) == 0:\n",
    "    print('\\n⚠️ データがありません！')\n",
    "    print('data/person_a/ と data/person_b/ に顔画像を配置してください。')\n",
    "else:\n",
    "    print('\\n✅ データが見つかりました！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル画像を表示\n",
    "def show_sample_images(dir_a, dir_b, num_samples=4):\n",
    "    \"\"\"各人物のサンプル画像を表示\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(3*num_samples, 6))\n",
    "    \n",
    "    extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    images_a = [f for f in dir_a.iterdir() if f.suffix.lower() in extensions]\n",
    "    images_b = [f for f in dir_b.iterdir() if f.suffix.lower() in extensions]\n",
    "    \n",
    "    samples_a = random.sample(images_a, min(num_samples, len(images_a)))\n",
    "    samples_b = random.sample(images_b, min(num_samples, len(images_b)))\n",
    "    \n",
    "    for i, img_path in enumerate(samples_a):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('人物A', fontsize=12)\n",
    "    \n",
    "    for i, img_path in enumerate(samples_b):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('人物B', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# データがあれば表示\n",
    "if count_images(data_a_dir) > 0 and count_images(data_b_dir) > 0:\n",
    "    show_sample_images(data_a_dir, data_b_dir)\n",
    "else:\n",
    "    print('データを配置後、このセルを再実行してください。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. モデルの学習\n",
    "\n",
    "ノートブック内で学習を実行できます。ただし、長時間の学習はターミナルで実行することを推奨します。\n",
    "\n",
    "### ターミナルでの実行（推奨）\n",
    "```bash\n",
    "# 軽量版（高速、教育用）\n",
    "python train.py --data-a data/person_a --data-b data/person_b --epochs 50\n",
    "\n",
    "# 品質重視版（Perceptual Loss有効）\n",
    "python train.py --data-a data/person_a --data-b data/person_b --epochs 200 --perceptual\n",
    "```\n",
    "\n",
    "### ノートブック内での学習\n",
    "以下のセルで短いデモ学習を実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブック内での短い学習（デモ用、5エポック）\n",
    "from src.data import create_dataloaders\n",
    "from src.training import DeepfakeTrainer\n",
    "\n",
    "# データがあるか確認\n",
    "if count_images(data_a_dir) == 0 or count_images(data_b_dir) == 0:\n",
    "    print('❌ データがありません。先にデータを準備してください。')\n",
    "else:\n",
    "    print('=== 短いデモ学習（5エポック） ===')\n",
    "    print('注意: 本格的な学習はターミナルで実行してください。\\n')\n",
    "    \n",
    "    # データローダー作成\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        dir_a=str(data_a_dir),\n",
    "        dir_b=str(data_b_dir),\n",
    "        batch_size=8,\n",
    "        image_size=128,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    # モデル作成\n",
    "    demo_model = DeepfakeModel(input_size=128, latent_dim=128)\n",
    "    \n",
    "    # トレーナー作成（一時ディレクトリに保存）\n",
    "    demo_trainer = DeepfakeTrainer(\n",
    "        model=demo_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        checkpoint_dir='../checkpoints_demo',\n",
    "        log_dir='../runs_demo',\n",
    "        learning_rate=1e-4,\n",
    "        use_perceptual=False,\n",
    "    )\n",
    "    \n",
    "    # 5エポックだけ学習\n",
    "    demo_trainer.train(num_epochs=5)\n",
    "    print('\\n✅ デモ学習完了！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 顔交換テスト\n",
    "\n",
    "学習済みモデルを使って、実際に顔交換を行います。\n",
    "\n",
    "### チェックポイントの場所\n",
    "- `checkpoints/best.pt` - 最も良いモデル（Val Loss基準）\n",
    "- `checkpoints/latest.pt` - 最新のモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルの読み込み\n",
    "checkpoint_path = Path('../checkpoints/best.pt')\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    print('❌ 学習済みモデルが見つかりません。')\n",
    "    print('先に学習を実行してください：')\n",
    "    print('  python train.py --data-a data/person_a --data-b data/person_b')\n",
    "else:\n",
    "    # モデル作成と重みの読み込み\n",
    "    swap_model = DeepfakeModel(input_size=128, latent_dim=128)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    swap_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    swap_model.to(device)\n",
    "    swap_model.eval()\n",
    "    \n",
    "    print('✅ モデル読み込み完了')\n",
    "    print(f\"  エポック: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  損失: {checkpoint.get('val_loss', checkpoint.get('loss', 'N/A')):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顔交換を実行して可視化\n",
    "def load_image_tensor(image_path, size=128):\n",
    "    \"\"\"画像をテンソルに変換\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"テンソルをPIL画像に変換\"\"\"\n",
    "    img = tensor.squeeze(0).cpu().detach()\n",
    "    img = img.clamp(0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((img * 255).astype('uint8'))\n",
    "\n",
    "def visualize_face_swap(model, img_a_path, img_b_path, device):\n",
    "    \"\"\"顔交換の結果を可視化\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 画像読み込み\n",
    "        img_a = load_image_tensor(img_a_path).to(device)\n",
    "        img_b = load_image_tensor(img_b_path).to(device)\n",
    "        \n",
    "        # モデルで処理\n",
    "        outputs = model.get_training_outputs(img_a, img_b)\n",
    "        \n",
    "        # 上段: 人物A\n",
    "        axes[0, 0].imshow(tensor_to_image(img_a))\n",
    "        axes[0, 0].set_title('Person A (Input)')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(tensor_to_image(outputs['recon_a']))\n",
    "        axes[0, 1].set_title('Person A (Reconstructed)')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(tensor_to_image(outputs['swap_a_to_b']))\n",
    "        axes[0, 2].set_title('A->B (A expression on B face)')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        # 下段: 人物B\n",
    "        axes[1, 0].imshow(tensor_to_image(img_b))\n",
    "        axes[1, 0].set_title('Person B (Input)')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(tensor_to_image(outputs['recon_b']))\n",
    "        axes[1, 1].set_title('Person B (Reconstructed)')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(tensor_to_image(outputs['swap_b_to_a']))\n",
    "        axes[1, 2].set_title('B->A (B expression on A face)')\n",
    "        axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Deepfake Face Swap Demo', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# モデルとデータがあれば実行\n",
    "if checkpoint_path.exists() and count_images(data_a_dir) > 0:\n",
    "    # ランダムな画像を選択\n",
    "    extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    images_a = [f for f in data_a_dir.iterdir() if f.suffix.lower() in extensions]\n",
    "    images_b = [f for f in data_b_dir.iterdir() if f.suffix.lower() in extensions]\n",
    "    \n",
    "    img_a_path = random.choice(images_a)\n",
    "    img_b_path = random.choice(images_b)\n",
    "    \n",
    "    print(f'Using images:')\n",
    "    print(f'  Person A: {img_a_path.name}')\n",
    "    print(f'  Person B: {img_b_path.name}')\n",
    "    print()\n",
    "    \n",
    "    visualize_face_swap(swap_model, str(img_a_path), str(img_b_path), device)\n",
    "else:\n",
    "    print('Prepare model and data, then re-run this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のサンプルで顔交換を比較\n",
    "def visualize_multiple_swaps(model, dir_a, dir_b, device, num_samples=4):\n",
    "    \"\"\"複数の画像ペアで顔交換を比較\"\"\"\n",
    "    extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    images_a = [f for f in dir_a.iterdir() if f.suffix.lower() in extensions]\n",
    "    images_b = [f for f in dir_b.iterdir() if f.suffix.lower() in extensions]\n",
    "    \n",
    "    samples_a = random.sample(images_a, min(num_samples, len(images_a)))\n",
    "    samples_b = random.sample(images_b, min(num_samples, len(images_b)))\n",
    "    \n",
    "    fig, axes = plt.subplots(4, num_samples, figsize=(3*num_samples, 12))\n",
    "    \n",
    "    row_labels = ['Person A (Input)', 'A->B Swap', 'Person B (Input)', 'B->A Swap']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img_a = load_image_tensor(str(samples_a[i])).to(device)\n",
    "            img_b = load_image_tensor(str(samples_b[i])).to(device)\n",
    "            outputs = model.get_training_outputs(img_a, img_b)\n",
    "            \n",
    "            results = [\n",
    "                img_a,\n",
    "                outputs['swap_a_to_b'],\n",
    "                img_b,\n",
    "                outputs['swap_b_to_a'],\n",
    "            ]\n",
    "            \n",
    "            for row, tensor in enumerate(results):\n",
    "                axes[row, i].imshow(tensor_to_image(tensor))\n",
    "                axes[row, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[row, i].set_ylabel(row_labels[row], fontsize=10, \n",
    "                                           rotation=0, labelpad=70, ha='right')\n",
    "    \n",
    "    plt.suptitle('Multiple Face Swap Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# モデルとデータがあれば実行\n",
    "if checkpoint_path.exists() and count_images(data_a_dir) > 0:\n",
    "    visualize_multiple_swaps(swap_model, data_a_dir, data_b_dir, device, num_samples=4)\n",
    "else:\n",
    "    print('Prepare model and data, then re-run this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "| セクション | 内容 |\n",
    "|-----------|------|\n",
    "| 1-6 | Deepfake技術の概念理解 |\n",
    "| 7 | データの準備と確認 |\n",
    "| 8 | モデルの学習（デモ/本格） |\n",
    "| 9 | 顔交換テストと可視化 |\n",
    "\n",
    "### 品質改善のヒント\n",
    "\n",
    "1. **より多くのデータ**: 各人物1000枚以上で品質向上\n",
    "2. **Perceptual Loss**: `--perceptual`オプションで構造的類似性を改善\n",
    "3. **より長い学習**: 200エポック以上で収束\n",
    "4. **高解像度**: `--image-size 256`で細部を表現（遅くなる）\n",
    "\n",
    "### 参考コマンド\n",
    "\n",
    "```bash\n",
    "# 高品質学習（推奨）\n",
    "python train.py --data-a data/person_a --data-b data/person_b \\\n",
    "    --epochs 200 --perceptual --batch-size 8\n",
    "\n",
    "# TensorBoardで学習監視\n",
    "tensorboard --logdir runs\n",
    "\n",
    "# 顔交換デモ実行\n",
    "python demo_swap.py\n",
    "```\n",
    "\n",
    "### 倫理的注意\n",
    "\n",
    "- ✅ 教育・研究目的のみ使用\n",
    "- ✅ 同意を得た人物の画像のみ使用\n",
    "- ❌ なりすまし・詐欺行為は厳禁\n",
    "- ❌ 他人を傷つける目的での使用禁止"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
